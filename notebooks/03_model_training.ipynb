{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'compute_class_weight' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[19], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m y_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/processed/y_test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Calculate class weights for imbalanced data\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_class_weight\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m, classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y_train), y\u001b[38;5;241m=\u001b[39my_train)\n\u001b[0;32m     20\u001b[0m class_weight_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y_train), class_weights))\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCalculated class weights: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_weight_dict\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'compute_class_weight' is not defined"
          ]
        }
      ],
      "source": [
        "# model_training.ipynb\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score, precision_score, recall_score\n",
        "import joblib\n",
        "\n",
        "# Load preprocessed data\n",
        "X_train = pd.read_csv(\"data/processed/X_train.csv\")\n",
        "y_train = pd.read_csv(\"data/processed/y_train.csv\").squeeze()\n",
        "X_test = pd.read_csv(\"data/processed/X_test.csv\")\n",
        "y_test = pd.read_csv(\"data/processed/y_test.csv\").squeeze()\n",
        "\n",
        "# Calculate class weights for imbalanced data\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
        "print(f\"\\nCalculated class weights: {class_weight_dict}\")\n",
        "\n",
        "# Define models with and without class weights\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Logistic Regression (Balanced)': LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'),\n",
        "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
        "    'Random Forest (Balanced)': RandomForestClassifier(random_state=42, n_estimators=100, class_weight='balanced'),\n",
        "    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss'),\n",
        "    'XGBoost (Balanced)': xgb.XGBClassifier(random_state=42, eval_metric='logloss', \n",
        "                                           scale_pos_weight=class_weights[0]/class_weights[1])\n",
        "}\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name):\n",
        "    \"\"\"Evaluate a model and return metrics\"\"\"\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "    \n",
        "    # Class-specific metrics\n",
        "    precision_per_class = precision_score(y_test, y_pred, average=None)\n",
        "    recall_per_class = recall_score(y_test, y_pred, average=None)\n",
        "    f1_per_class = f1_score(y_test, y_pred, average=None)\n",
        "    \n",
        "    if y_pred_proba is not None:\n",
        "        auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    else:\n",
        "        auc = None\n",
        "    \n",
        "    return {\n",
        "        'Model': model_name,\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1-Score': f1,\n",
        "        'AUC': auc,\n",
        "        'Precision_Declined': precision_per_class[0],\n",
        "        'Recall_Declined': recall_per_class[0],\n",
        "        'F1_Declined': f1_per_class[0],\n",
        "        'Precision_Approved': precision_per_class[1],\n",
        "        'Recall_Approved': recall_per_class[1],\n",
        "        'F1_Approved': f1_per_class[1],\n",
        "        'Predictions': y_pred,\n",
        "        'Probabilities': y_pred_proba,\n",
        "        'Model_Object': model\n",
        "    }\n",
        "\n",
        "# Store all results\n",
        "all_results = []\n",
        "\n",
        "# Train and evaluate all models\n",
        "print(\"Training and evaluating models...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"Training {model_name}...\")\n",
        "    \n",
        "    # Evaluate model\n",
        "    result = evaluate_model(model, X_train_scaled, y_train, X_test_scaled, y_test, model_name)\n",
        "    all_results.append(result)\n",
        "    \n",
        "    print(f\"  Overall Metrics:\")\n",
        "    print(f\"    Accuracy: {result['Accuracy']:.4f}\")\n",
        "    print(f\"    Precision: {result['Precision']:.4f}\")\n",
        "    print(f\"    Recall: {result['Recall']:.4f}\")\n",
        "    print(f\"    F1-Score: {result['F1-Score']:.4f}\")\n",
        "    if result['AUC']:\n",
        "        print(f\"    AUC: {result['AUC']:.4f}\")\n",
        "    \n",
        "    print(f\"  Class-specific Metrics:\")\n",
        "    print(f\"    Declined - Precision: {result['Precision_Declined']:.4f}, Recall: {result['Recall_Declined']:.4f}, F1: {result['F1_Declined']:.4f}\")\n",
        "    print(f\"    Approved - Precision: {result['Precision_Approved']:.4f}, Recall: {result['Recall_Approved']:.4f}, F1: {result['F1_Approved']:.4f}\")\n",
        "    print()\n",
        "\n",
        "# =================================================================================\n",
        "# 6. RESULTS COMPARISON AND VISUALIZATION\n",
        "# =================================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"RESULTS COMPARISON\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame(all_results)\n",
        "results_summary = results_df[['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC', \n",
        "                             'Precision_Declined', 'Recall_Declined', 'F1_Declined',\n",
        "                             'Precision_Approved', 'Recall_Approved', 'F1_Approved']].copy()\n",
        "\n",
        "print(\"Complete Results Summary:\")\n",
        "print(results_summary.round(4))\n",
        "\n",
        "# Find best model for each metric\n",
        "print(\"\\nBest Models by Metric:\")\n",
        "for metric in ['Accuracy', 'F1-Score', 'F1_Declined', 'F1_Approved']:\n",
        "    best_idx = results_summary[metric].idxmax()\n",
        "    best_model = results_summary.loc[best_idx]\n",
        "    print(f\"{metric}: {best_model['Model']} (Score: {best_model[metric]:.4f})\")\n",
        "\n",
        "# Visualize overall results\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Overall metrics\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "for i, metric in enumerate(metrics):\n",
        "    ax = axes[i//2, i%2]\n",
        "    results_summary.plot(x='Model', y=metric, kind='bar', ax=ax, color='skyblue')\n",
        "    ax.set_title(f'{metric} Comparison')\n",
        "    ax.set_xlabel('Model')\n",
        "    ax.set_ylabel(metric)\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    ax.legend().remove()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Class-specific performance comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Declined class performance\n",
        "declined_metrics = ['Precision_Declined', 'Recall_Declined', 'F1_Declined']\n",
        "declined_data = results_summary[['Model'] + declined_metrics].set_index('Model')\n",
        "declined_data.plot(kind='bar', ax=axes[0], color=['red', 'orange', 'pink'])\n",
        "axes[0].set_title('Performance on Declined Class')\n",
        "axes[0].set_xlabel('Model')\n",
        "axes[0].set_ylabel('Score')\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "axes[0].legend(['Precision', 'Recall', 'F1-Score'])\n",
        "\n",
        "# Approved class performance\n",
        "approved_metrics = ['Precision_Approved', 'Recall_Approved', 'F1_Approved']\n",
        "approved_data = results_summary[['Model'] + approved_metrics].set_index('Model')\n",
        "approved_data.plot(kind='bar', ax=axes[1], color=['green', 'lightgreen', 'darkgreen'])\n",
        "axes[1].set_title('Performance on Approved Class')\n",
        "axes[1].set_xlabel('Model')\n",
        "axes[1].set_ylabel('Score')\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "axes[1].legend(['Precision', 'Recall', 'F1-Score'])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# =================================================================================\n",
        "# 7. CONFUSION MATRICES FOR ALL MODELS\n",
        "# =================================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"CONFUSION MATRICES\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Plot confusion matrices for all models\n",
        "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, result in enumerate(all_results):\n",
        "    if i < len(axes):\n",
        "        cm = confusion_matrix(y_test, result['Predictions'])\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n",
        "                   xticklabels=target_encoder.classes_, \n",
        "                   yticklabels=target_encoder.classes_)\n",
        "        axes[i].set_title(f'{result[\"Model\"]}')\n",
        "        axes[i].set_xlabel('Predicted')\n",
        "        axes[i].set_ylabel('Actual')\n",
        "\n",
        "# Remove empty subplots\n",
        "for j in range(len(all_results), len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Detailed classification reports for best models\n",
        "best_overall_idx = results_summary['F1-Score'].idxmax()\n",
        "best_declined_idx = results_summary['F1_Declined'].idxmax()\n",
        "\n",
        "print(f\"\\nBest Overall Model: {results_summary.loc[best_overall_idx]['Model']}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, all_results[best_overall_idx]['Predictions'], \n",
        "                          target_names=target_encoder.classes_))\n",
        "\n",
        "print(f\"\\nBest Model for Declined Class: {results_summary.loc[best_declined_idx]['Model']}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, all_results[best_declined_idx]['Predictions'], \n",
        "                          target_names=target_encoder.classes_))\n",
        "\n",
        "# =================================================================================\n",
        "# 8. ROC CURVES COMPARISON\n",
        "# =================================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ROC CURVES COMPARISON\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown', 'pink']\n",
        "\n",
        "for i, result in enumerate(all_results):\n",
        "    if result['Probabilities'] is not None:\n",
        "        fpr, tpr, _ = roc_curve(y_test, result['Probabilities'])\n",
        "        auc_score = result['AUC']\n",
        "        plt.plot(fpr, tpr, color=colors[i % len(colors)], \n",
        "                label=f\"{result['Model']} (AUC = {auc_score:.3f})\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curves Comparison')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”§ Recommended Solutions:\n",
        "#### Step 1: Check for Data Leakage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features highly correlated with target:\n",
            "Series([], dtype: float64)\n"
          ]
        }
      ],
      "source": [
        "# Check if any features are perfectly correlated with target\n",
        "correlation_with_target = X_train.corrwith(pd.Series(y_train))\n",
        "print(\"Features highly correlated with target:\")\n",
        "print(correlation_with_target[abs(correlation_with_target) > 0.8])\n",
        "\n",
        "# Check for identical values between features and target\n",
        "for col in X_train.columns:\n",
        "    if X_train[col].dtype in ['int64', 'float64']:\n",
        "        correlation = X_train[col].corr(pd.Series(y_train))\n",
        "        if abs(correlation) > 0.9:\n",
        "            print(f\"High correlation found: {col} -> {correlation:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Add Regularization and Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ENG.YAHYA\\Desktop\\projects\\Loan Approval Classification\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\ENG.YAHYA\\Desktop\\projects\\Loan Approval Classification\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\ENG.YAHYA\\Desktop\\projects\\Loan Approval Classification\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\ENG.YAHYA\\Desktop\\projects\\Loan Approval Classification\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "c:\\Users\\ENG.YAHYA\\Desktop\\projects\\Loan Approval Classification\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Logistic Regression:\n",
            "CV F1 scores: [0.7779981  0.76848232 0.7838443  0.79503739 0.78208441]\n",
            "Mean F1: 0.781 (+/- 0.017)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ENG.YAHYA\\Desktop\\projects\\Loan Approval Classification\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test F1: 0.920\n",
            "\n",
            "Random Forest:\n",
            "CV F1 scores: [1. 1. 1. 1. 1.]\n",
            "Mean F1: 1.000 (+/- 0.000)\n",
            "Test F1: 1.000\n",
            "\n",
            "XGBoost:\n",
            "CV F1 scores: [1. 1. 1. 1. 1.]\n",
            "Mean F1: 1.000 (+/- 0.000)\n",
            "Test F1: 1.000\n",
            "\n",
            "==================================================\n",
            "CROSS-VALIDATION RESULTS:\n",
            "                 Model  CV F1 Mean  CV F1 Std   Test F1\n",
            "0  Logistic Regression    0.781489   0.008612  0.920321\n",
            "1        Random Forest    1.000000   0.000000  1.000000\n",
            "2              XGBoost    1.000000   0.000000  1.000000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "\n",
        "# Updated models with regularization\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(\n",
        "        max_iter=1000, \n",
        "        random_state=42, \n",
        "        C=0.1,  # Add regularization\n",
        "        class_weight='balanced'  # Handle imbalance\n",
        "    ),\n",
        "    'Random Forest': RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=10,  # Limit depth to prevent overfitting\n",
        "        min_samples_split=20,\n",
        "        min_samples_leaf=10,\n",
        "        random_state=42,\n",
        "        class_weight='balanced'\n",
        "    ),\n",
        "    'XGBoost': xgb.XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,  # Limit depth\n",
        "        learning_rate=0.1,\n",
        "        reg_alpha=0.1,  # L1 regularization\n",
        "        reg_lambda=0.1,  # L2 regularization\n",
        "        scale_pos_weight=5,  # Handle imbalance (32705/6474 â‰ˆ 5)\n",
        "        random_state=42,\n",
        "        eval_metric='logloss'\n",
        "    )\n",
        "}\n",
        "\n",
        "# Use cross-validation instead of single train-test\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Cross-validation scores\n",
        "    f1_scores = cross_val_score(model, X_train, y_train, \n",
        "                               cv=cv, scoring=make_scorer(f1_score))\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"CV F1 scores: {f1_scores}\")\n",
        "    print(f\"Mean F1: {f1_scores.mean():.3f} (+/- {f1_scores.std() * 2:.3f})\")\n",
        "    \n",
        "    # Train on full training set and evaluate on test set\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    test_f1 = f1_score(y_test, y_pred)\n",
        "    print(f\"Test F1: {test_f1:.3f}\")\n",
        "    \n",
        "    results.append({\n",
        "        'Model': name,\n",
        "        'CV F1 Mean': f1_scores.mean(),\n",
        "        'CV F1 Std': f1_scores.std(),\n",
        "        'Test F1': test_f1\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"CROSS-VALIDATION RESULTS:\")\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
